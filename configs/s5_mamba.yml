
mamba:
    n_layer: 2
    d_model: 128
    vocab_size: 120
    rms_norm: True
    residual_in_fp32: True
    fused_add_norm:  True
    pad_vocab_size_multiple: 8
    ssm_cfg:
        d_state: 16
        d_conv: 2
        expand: 2

data:
    monoid: symmetric
    n: 5

train:
    num_iters: 500_000
    total_seq_length: 64
    batch_size: 512

optimizer:
    optim: AdamW
    lr: 0.0005
    weight_decay: 0.0

