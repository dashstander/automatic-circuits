{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5832d684-b3e9-4930-987d-b16528a12e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86750e2e-efdd-4da8-af56-0366914d3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_lengths(total_length, min_length, max_length, rng):\n",
    "    too_many = rng.integers(min_length, max_length, min_length * total_length)\n",
    "    sequence_lengths = too_many[np.cumsum(too_many) <= total_length]\n",
    "    diff = total_length - np.sum(sequence_lengths)\n",
    "    sequence_lengths = sequence_lengths.tolist()\n",
    "    if diff >= min_length and diff <= max_length:\n",
    "        sequence_lengths += [diff]\n",
    "    return sequence_lengths\n",
    "    \n",
    "\n",
    "def generate_cum_parity(total_seq_len: int, rng):\n",
    "    sep = np.array([2])\n",
    "    prob = rng.beta(2, 2)\n",
    "    bits = rng.choice(2, (total_seq_len - 1,), replace=True, p=[prob, 1. - prob])\n",
    "    parities = np.concatenate([sep, np.cumsum(bits) % 2])\n",
    "    bits = np.concatenate([sep, bits])\n",
    "    return bits, parities\n",
    "\n",
    "\n",
    "def generate_packed_parity(total_seq_length, min_seq_length=6, max_seq_length=30, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    sep = 2\n",
    "    sequence_lengths = get_seq_lengths(total_seq_length, min_seq_length, max_seq_length, rng)\n",
    "    assert sum(sequence_lengths) <= total_seq_length\n",
    "    [bits, parities] = list(zip(*[\n",
    "            generate_cum_parity(seq_len, stream) for seq_len, stream in zip(sequence_lengths, rng.spawn(len(sequence_lengths)))\n",
    "    ]))\n",
    "    bits = np.concatenate(bits)\n",
    "    parities = np.concatenate(parities)\n",
    "    diff = total_seq_length - len(parities)\n",
    "    bits = np.pad(bits, (0, diff), mode='constant', constant_values=(sep, sep))\n",
    "    parities = np.pad(parities, (0, diff), mode='constant', constant_values=(sep, sep))\n",
    "    return bits, parities\n",
    "\n",
    "\n",
    "def generate_fixed_parity(total_seq_length, seq_length=30, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    sep = 2\n",
    "    sequence_lengths = [seq_length] * (total_seq_length // seq_length)\n",
    "    assert sum(sequence_lengths) <= total_seq_length\n",
    "    [bits, parities] = list(zip(*[\n",
    "            generate_cum_parity(seq_len, stream) for seq_len, stream in zip(sequence_lengths, rng.spawn(len(sequence_lengths)))\n",
    "    ]))\n",
    "    bits = np.concatenate(bits)\n",
    "    parities = np.concatenate(parities)\n",
    "    diff = total_seq_length - len(parities)\n",
    "    bits = np.pad(bits, (0, diff), mode='constant', constant_values=(sep, sep))\n",
    "    parities = np.pad(parities, (0, diff), mode='constant', constant_values=(sep, sep))\n",
    "    return bits, parities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ac0b81-204c-42aa-99b6-fd557dc96089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "        \"d_model\": 64,\n",
    "        \"d_head\": 32,\n",
    "        \"n_heads\": 2,\n",
    "        \"d_mlp\": 256,\n",
    "        \"n_ctx\": 512,\n",
    "        \"n_layers\": 1,\n",
    "        \"d_vocab\": 4,\n",
    "        \"act_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "ckpt = torch.load('checkpoints/199999.pth')\n",
    "\n",
    "config = HookedTransformerConfig(**cfg)\n",
    "model = HookedTransformer(config)\n",
    "model.load_state_dict(ckpt['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c634ac-e0e6-4600-8484-057b65703232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb499d17-f0d0-4109-8233-b1215f58976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class CumulativeParityFixed(IterableDataset):\n",
    "    def __init__(self, total_sequence_length: int, sequence_length: int, batch_size: int, rng_seed: int = 0):\n",
    "        super().__init__()\n",
    "        self.total_sequence_length = total_sequence_length\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            all_rngs = [stream for stream in self.rng.spawn(worker_info.num_workers)]\n",
    "            rng = all_rngs[worker_info.id]\n",
    "        else:\n",
    "            rng = self.rng\n",
    "        while True:\n",
    "            [bits, parities] = list(zip(*[\n",
    "                generate_fixed_parity(\n",
    "                    self.total_sequence_length,\n",
    "                    self.sequence_length,\n",
    "                    stream\n",
    "                ) for stream in rng.spawn(self.batch_size)\n",
    "            ]))\n",
    "            yield torch.asarray(np.stack(bits, axis=0)), torch.asarray(np.stack(parities, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de81f97a-d9aa-4cd3-bf2b-584a0e166e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator51 = CumulativeParityFixed(512, 32, 512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d7145e0-b5e6-4080-9a9b-2e0d9d614da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data = iter(data_generator51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef4b98f6-8422-4e56-9653-2d4bd48eb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits, parities = next(iter_data)\n",
    "\n",
    "output = model(bits.to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7257f2cb-ac5d-4043-9221-0f8abfae5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "def seq2seq_cross_entropy_loss(logits, tokens, ignore_token=2):\n",
    "    log_probs = log_softmax(logits, dim=-1)\n",
    "    # Use torch.gather to find the log probs of the correct tokens\n",
    "    # Not using offsets because we're predicting the same token position, new _sequence\n",
    "    # None and [..., 0] needed because the tensor used in gather must have the same rank.\n",
    "    predicted_log_probs = log_probs[..., :, :].gather(\n",
    "        dim=-1, index=tokens[..., :, None]\n",
    "    )[..., 0]\n",
    "\n",
    "    log_probs = torch.where(\n",
    "        tokens != ignore_token,\n",
    "        predicted_log_probs,\n",
    "        torch.zeros_like(predicted_log_probs)\n",
    "    )\n",
    "\n",
    "    return -log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5111b64e-6450-4f74-a6ef-7709a4668f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5235, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_cross_entropy_loss(output, parities.to('cuda'), ignore_token=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d04f6a28-5072-4eef-9862-05accd97e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seq2seq_accuracy(logits, tokens):\n",
    "    predicted_tok = logits.argmax(dim=-1)\n",
    "    return (predicted_tok == tokens).to(torch.float32).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f92e6c61-f54c-43a2-b314-608a83d18f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6275, device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_accuracy(output, parities.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7147be0-e2ab-4d67-ab2d-5ddbea1e5ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0172ba01-3349-443e-89a6-63bce9b04a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "class CumulativeParityDataset:\n",
    "\n",
    "    def __init__(self, total_sequence_length: int, min_sequence_length: int, max_sequence_length: int, batch_size: int, rng_seed: int = 0):\n",
    "        super().__init__()\n",
    "        self.total_sequence_length = total_sequence_length\n",
    "        self.min_sequence_length = min_sequence_length\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    def next(self):\n",
    "        #worker_info = torch.utils.data.get_worker_info()\n",
    "        #if worker_info is not None:\n",
    "        #    all_rngs = [stream for stream in self.rng.spawn(worker_info.num_workers)]\n",
    "        #    rng = all_rngs[worker_info.id]\n",
    "        #else:\n",
    "        rng = self.rng\n",
    "        parities = [\n",
    "            generate_packed_parity(\n",
    "                self.total_sequence_length,\n",
    "                self.min_sequence_length,\n",
    "                self.max_sequence_length,\n",
    "                stream\n",
    "            ) for stream in rng.spawn(self.batch_size)\n",
    "        ]\n",
    "   \n",
    "        return torch.asarray(np.stack(parities, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361f1a35-8cc6-4b67-87a1-9dbf47674b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = CumulativeParityDataset(512, 8, 30, 512, 10)\n",
    "loader = DataLoader(dataset, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafc7e80-b835-4805-b376-ef01a1e18f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 0, 0,  ..., 1, 1, 3],\n",
       "        [3, 1, 1,  ..., 1, 0, 3],\n",
       "        [3, 1, 0,  ..., 1, 1, 3],\n",
       "        ...,\n",
       "        [3, 1, 0,  ..., 1, 0, 1],\n",
       "        [3, 0, 0,  ..., 3, 3, 3],\n",
       "        [3, 1, 0,  ..., 1, 0, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed7f7250-38ee-4b74-9e2a-58295dd4b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, num_steps, dataloader):\n",
    "\n",
    "    #loader = iter(dataloader)\n",
    "\n",
    "    with trange(num_steps) as t:\n",
    "        for i in t:\n",
    "            data = dataloader.next()\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(data.to('cuda:0'), return_type='loss')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                t.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd12a4f3-dd35-4467-8cff-adc5fe28101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup = 100\n",
    "num_steps = 10_000\n",
    "seed = 0\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    \"d_model\": 128,\n",
    "    \"d_head\": 32,\n",
    "    \"n_heads\": 2,\n",
    "    \"d_mlp\": 512,\n",
    "    \"n_ctx\": 512,\n",
    "    \"n_layers\": 1,\n",
    "    \"d_vocab\": 4,\n",
    "    \"act_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "config = HookedTransformerConfig(**cfg)\n",
    "model = HookedTransformer(config)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=num_warmup)\n",
    "annealing = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(num_steps - num_warmup), eta_min=1.0e-6)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [warmup, annealing], milestones=[num_warmup])\n",
    "\n",
    "dataset = CumulativeParityDataset(512, 6, 32, 512, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57967878-da9d-4704-bf7d-0356c47fbf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63462facaa624469b8c336a909396803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, num_steps, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5b14d-2ce1-4768-a673-03de7d09d3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
